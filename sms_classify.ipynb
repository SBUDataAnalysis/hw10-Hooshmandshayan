{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression for SMS spam classification\n",
    "\n",
    "\n",
    "Each line of the data file `sms.txt`\n",
    "contains a label---either \"spam\" or \"ham\" (i.e. non-spam)---followed\n",
    "by a text message. Here are a few examples (line breaks added for readability):\n",
    "\n",
    "    ham     Ok lar... Joking wif u oni...\n",
    "    ham     Nah I don't think he goes to usf, he lives around here though\n",
    "    spam    Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005.\n",
    "            Text FA to 87121 to receive entry question(std txt rate)\n",
    "            T&C's apply 08452810075over18's\n",
    "    spam    WINNER!! As a valued network customer you have been\n",
    "            selected to receivea £900 prize reward! To claim\n",
    "            call 09061701461. Claim code KL341. Valid 12 hours only.\n",
    "\n",
    "To create features suitable for logistic regression, use tools from the ``sklearn.feature_extraction.text``:\n",
    "\n",
    "* Convert words to lowercase.\n",
    "* Remove punctuation and special characters (but convert the \\$ and\n",
    "  £ symbols to special tokens and keep them, because these are useful for predicting spam).\n",
    "* Create a dictionary containing the 3000 words that appeared\n",
    "  most frequently in the entire set of messages.\n",
    "* Encode each message as a vector $\\mathbf{x}^{(i)} \\in\n",
    "  \\mathbb{R}^{3000}$. The entry $x^{(i)}_j$ is equal to the\n",
    "  number of times the $j$th word in the dictionary appears in that\n",
    "  message.\n",
    "* Discard some ham messages to have an\n",
    "  equal number of spam and ham messages.\n",
    "* Split data into a training set of 1000 messages and a\n",
    "  test set of 400 messages.\n",
    "  \n",
    "Follow the instructions below to complete the implementation. You will be asked to: \n",
    "\n",
    "* write a code to implement logestic regression algorithm (you can use sklearn library for this but it affects your score.)\n",
    "* Make predictions and report the accuracy on the test set\n",
    "* Test out the classifier on a few of your own text messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build Logisitc Regression classifier\n",
    "for this part you can use Andrew Ng course for machine learning week3 in coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, X, Y, iter_num = 10000, learning_rate = 0.01):\n",
    "\n",
    "        m = X.shape[1]\n",
    "\n",
    "        self.X = np.concatenate((np.ones((1,m)), X), axis=0)\n",
    "        self.Y = Y\n",
    "        \n",
    "        n_x = self.X.shape[0]\n",
    "        \n",
    "        self.theta = np.zeros((1,n_x))\n",
    "\n",
    "        for _ in range(iter_num):\n",
    "            grads = self.compute_grads()\n",
    "            self.theta -= learning_rate * grads\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_grads(self):\n",
    "        m = self.X.shape[1]\n",
    "        return (1/m) * np.dot(np.subtract(self.sigmoid(np.dot(self.theta, self.X)), self.Y),self.X.T)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.concatenate((np.ones((1,X_test.shape[1])), X_test), axis=0)\n",
    "        predicted_Y = self.sigmoid(np.dot(self.theta, X_test))\n",
    "        return (predicted_Y > 0.5).astype(int)\n",
    "\n",
    "    def accuracy(self, X_test, Y_test):\n",
    "      Y_pred = self.predict(X_test)\n",
    "      corrects = np.sum(Y_pred == Y_test)\n",
    "      return corrects/Y_test.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prep data\n",
    "using provided construction load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'spam', 'ham']\n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'U dun say so early hor... U c already then say...', \"Nah I don't think he goes to usf, he lives around here though\"]\n",
      "5575\n",
      "5574\n",
      "['ham', 'ham', '']\n",
      "  label                                           msg_list\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m     text\u001b[39m=\u001b[39m[word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m combined_df[\u001b[39m'\u001b[39m\u001b[39mmsg_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(text)\n\u001b[1;32m---> 18\u001b[0m combined_df[\u001b[39m'\u001b[39m\u001b[39mmsg_clean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m combined_df[\u001b[39m'\u001b[39;49m\u001b[39mmsg_list\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: clean_text(x\u001b[39m.\u001b[39;49mlower()))\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstemming\u001b[39m(tokenized_text):\n\u001b[0;32m     21\u001b[0m     text \u001b[39m=\u001b[39m [pd\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokenized_text]\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4322'>4323</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4323'>4324</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4324'>4325</a>\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4327'>4328</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4328'>4329</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4329'>4330</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4330'>4331</a>\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4331'>4332</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4430'>4431</a>\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4431'>4432</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/series.py?line=4432'>4433</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1083'>1084</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1084'>1085</a>\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1085'>1086</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m-> <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1087'>1088</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1136'>1137</a>\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1137'>1138</a>\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1138'>1139</a>\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1139'>1140</a>\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1140'>1141</a>\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1141'>1142</a>\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1142'>1143</a>\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1143'>1144</a>\u001b[0m             values,\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1144'>1145</a>\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1145'>1146</a>\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1146'>1147</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1148'>1149</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1149'>1150</a>\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1150'>1151</a>\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Shayan/AppData/Local/Programs/Python/Python310/lib/site-packages/pandas/core/apply.py?line=1151'>1152</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Shayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m     text\u001b[39m=\u001b[39m[word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m combined_df[\u001b[39m'\u001b[39m\u001b[39mmsg_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(text)\n\u001b[1;32m---> 18\u001b[0m combined_df[\u001b[39m'\u001b[39m\u001b[39mmsg_clean\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m combined_df[\u001b[39m'\u001b[39m\u001b[39mmsg_list\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: clean_text(x\u001b[39m.\u001b[39;49mlower()))\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstemming\u001b[39m(tokenized_text):\n\u001b[0;32m     21\u001b[0m     text \u001b[39m=\u001b[39m [pd\u001b[39m.\u001b[39mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokenized_text]\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mclean_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_text\u001b[39m(text):\n\u001b[0;32m     14\u001b[0m     text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m text \u001b[39mif\u001b[39;00m c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string\u001b[39m.\u001b[39mpunctuation])\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     text\u001b[39m=\u001b[39m[word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m combined_df[\u001b[39m'\u001b[39m\u001b[39mmsg_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(text)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_text\u001b[39m(text):\n\u001b[0;32m     14\u001b[0m     text\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m text \u001b[39mif\u001b[39;00m c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m string\u001b[39m.\u001b[39mpunctuation])\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     text\u001b[39m=\u001b[39m[word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m combined_df[\u001b[39m'\u001b[39m\u001b[39mmsg_list\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(text)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "stopwords = nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter , Sequence\n",
    "import string\n",
    "\n",
    "ps = PorterStemmer()\n",
    "raw_data = open('sms.txt').read()\n",
    "parsed_data = raw_data.replace('\\t','\\n').split('\\n')\n",
    "label_list = parsed_data[0::2]\n",
    "msg_list = parsed_data[1::2]\n",
    "print(label_list[0:4])\n",
    "print(msg_list[0:5]) \n",
    "print(len(label_list))\n",
    "print(len(msg_list))\n",
    "print(label_list[-3:])\n",
    "combined_df = pd.DataFrame({'label' : label_list[:-1], 'msg_list' : msg_list})\n",
    "print(combined_df.head())\n",
    "\n",
    "def clean_text(text):\n",
    "    for punc in string.punctuation:\n",
    "        text = text.replace(punc, \"\")\n",
    "    text = text.split(\" \")\n",
    "    text=[word for word in text if word not in stopwords.words('english')]\n",
    "    return text\n",
    " \n",
    "combined_df['msg_list'] = combined_df['msg_list'].apply(lambda x: clean_text(x.lower()))\n",
    "\n",
    "def stem_and_tokenize(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "combined_df['msg_list'] = combined_df['msg_list'].apply(lambda x: stem_and_tokenize(x))\n",
    "clean_msg = list(combined_df.msg_list)\n",
    "\n",
    "dictionary = []\n",
    "for i in clean_msg:\n",
    "  dictionary.extend(i)\n",
    "words_occurences = dict(Counter(dictionary))\n",
    "\n",
    "n_x = 500\n",
    "words_occurences = dict(sorted(words_occurences.items(), key=lambda item: item[1], reverse=True)[:n_x])\n",
    "corpus = list(words_occurences.keys())\n",
    "\n",
    "X = np.zeros((n_x, len(clean_msg)))\n",
    "for i in range(len(clean_msg)):\n",
    "  X[:, i] = [clean_msg[i].count(x) for x in corpus]\n",
    "X = X.T\n",
    "print(X.shape)\n",
    "\n",
    "Y=[1 if i=='spam' else 0 for i in combined_df.label]\n",
    "Y = np.array(Y)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train logistic regresion model\n",
    "Using the logestic Regression method, train the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)\n",
    "model = LogisticRegression(X_train.T, y_train, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on test set\n",
    "Use the model fit in the previous cell to make predictions on the test set and compute the accuracy (percentage of messages in the test set that are classified correctly). You should be able to get accuracy above 95%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.accuracy(X_test.T, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model parameters\n",
    "find which words are most common in spam and ham messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.theta[0,1:]\n",
    "spam_words = sorted(range(len(weights)), key=lambda i: weights[i], reverse=True)[:5]\n",
    "ham_words = sorted(range(len(weights)), key=lambda i: weights[i])[:5]\n",
    "\n",
    "print(\"Top spam words:\")\n",
    "for i in spam_words:\n",
    "  print(corpus[i])\n",
    "print(\"\\n\")\n",
    "print(\"Top ham words:\")\n",
    "for i in ham_words:\n",
    "  print(corpus[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Make a prediction on new messages\n",
    "Type a few of your own messages in below and make predictions. Are they ham or spam? Do the predictions make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "bcad1ea371d62b1cf8352251dd39181dceca52c49a88b3479cefcf4ffab73788"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
